Introduction to LCEL and LangGraph: LangChain Powered RAG
In the following notebook we're going to focus on learning how to navigate and build useful applications using LangChain, specifically LCEL, and how to integrate different APIs together into a coherent RAG application!

In the notebook, you'll complete the following Tasks:

ü§ù Breakout Room #1:

Install required libraries
Set Environment Variables
Initialize a Simple Chain using LCEL
Implement Naive RAG using LCEL
Implement Simple RAG using LCEL
ü§ù Breakout Room #2:

Install LangGraph
Understanding States and Nodes
Building a Basic Graph
Implementing a Simple RAG Graph
Extending the Graph with Complex Flows
Let's get started!

ü§ù Breakout Room #1
Installing Required Libraries
One of the key features of LangChain v0.2.0 is the compartmentalization of the various LangChain ecosystem packages and added stability.

Instead of one all encompassing Python package - LangChain has a core package and a number of additional supplementary packages.

We'll start by grabbing all of our LangChain related packages!

NOTE: DO NOT RUN THIS CELL IF YOU ARE RUNNING THIS NOTEBOOK LOCALLY

#!pip install -qU langchain==0.3.15 langchain-core==0.3.31 langchain-community==0.3.15 langchain-openai==0.3.1 langchain-qdrant==0.2.0 qdrant-client==1.13.2 tiktoken pymupdf==1.25.2
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0.0/1.0 MB ? eta -:--:--
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∏‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0.3/1.0 MB 11.4 MB/s eta 0:00:01
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ï∏ 1.0/1.0 MB 18.6 MB/s eta 0:00:01
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1.0/1.0 MB 14.1 MB/s eta 0:00:00
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0.0/412.2 kB ? eta -:--:--
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 412.2/412.2 kB 32.1 MB/s eta 0:00:00
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2.5/2.5 MB 67.6 MB/s eta 0:00:00
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 54.3/54.3 kB 4.0 MB/s eta 0:00:00
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1.2/1.2 MB 46.4 MB/s eta 0:00:00
   ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 50.8/50.8 kB 4.1 MB/s eta 0:00:00
Set Environment Variables
We'll be leveraging OpenAI's suite of APIs - so we'll set our OPENAI_API_KEY env variable here!

import os
import getpass

os.environ["OPENAI_API_KEY"] = getpass.getpass("OpenAI API Key:")
Initialize a Simple Chain using LCEL
The first thing we'll do is familiarize ourselves with LCEL and the specific ins and outs of how we can use it!

LLM Orchestration Tool (LangChain)
Let's dive right into LangChain!

The first thing we want to do is create an object that lets us access OpenAI's gpt-4o model.

from langchain_openai import ChatOpenAI

openai_chat_model = ChatOpenAI(model="gpt-4o-mini")
‚ùì Question #1:
What other models could we use, and how would the above code change?

HINT: Check out this page to find the answer!

Prompt Template
Now, we'll set up a prompt template - more specifically a ChatPromptTemplate. This will let us build a prompt we can modify when we call our LLM!

from langchain_core.prompts import ChatPromptTemplate

system_template = "You are a legendary and mythical Wizard. You speak in riddles and make obscure and pun-filled references to exotic cheeses."
human_template = "{content}"

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", system_template),
    ("human", human_template)
])
Our First Chain
Now we can set up our first chain!

A chain is simply two components that feed directly into eachother in a sequential fashion!

You'll notice that we're using the pipe operator | to connect our chat_prompt to our llm.

This is a simplified method of creating chains and it leverages the LangChain Expression Language, or LCEL.

You can read more about it here, but there a few features we should be aware of out of the box (taken directly from LangChain's documentation linked above):

Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.

Fallbacks The non-determinism of LLMs makes it important to be able to handle errors gracefully. With LCEL you can easily attach fallbacks to any chain.

Parallelism Since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.

In the following code cell we have two components:

chat_prompt, which is a formattable ChatPromptTemplate that contains a system message and a human message.
openai_chat_model, which is a LangChain Runnable wrapped OpenAI client.
We'd like to be able to pass our own content (as found in our human_template) and then have the resulting message pair sent to our model and responded to!

chain = chat_prompt | openai_chat_model
Notice the pattern here:

We invoke our chain with the dict {"content" : "Hello world!"}.

It enters our chain:

{"content" : "Hello world!"} -> invoke() -> chat_prompt

Our chat_prompt returns a PromptValue, which is the formatted prompt. We then "pipe" the output of our chat_prompt into our llm.

PromptValue -> | -> llm

Our llm then takes the list of messages and provides an output which is return as a str!

print(chain.invoke({"content": "Hello world!"}))
content='Ah, greetings, seeker of truths profound! In the realm where brie dreams of becoming a wheel of fortune, what mystery dost thou wish to unfold? Speak, and let us cheddar our thoughts in joyous camaraderie! üßô\u200d‚ôÇÔ∏èüßÄ' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 38, 'total_tokens': 92, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b376dfbbd5', 'finish_reason': 'stop', 'logprobs': None} id='run-d7f6d8b9-2254-4069-ac54-64fc5d5904ff-0' usage_metadata={'input_tokens': 38, 'output_tokens': 54, 'total_tokens': 92, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}
Let's try it out with a different prompt!

chain.invoke({"content" : "Could I please have some advice on how to become a better Python Programmer?"})
AIMessage(content='Ah, young seeker of the Python‚Äôs arcane paths, gather \'round, and lend thine ear to my ponderous words, like the creamy layers of a fine Brie unfolding upon a crusty baguette.\n\nTo summon mastery in the serpentine language of Python, heed these enchanted crumbs of wisdom, each resembling a delightful morsel of fromage:\n\n1. **Mature Like a Gruy√®re**: Repeat after me: "Practice makes perfect." Much like the aging process of Swiss cheese, engage consistently with Python code. Spend time every day tinkering, experimenting, forming connections‚Äîtill your skills burst forth like holes in true Swiss delight.\n\n2. **Forge Thy Syntax as Gouda**: Learn the language‚Äôs syntax like the fine texture of aged Gouda. Seek out the secrets hidden within‚Äîcomprehend the use of loops, conditionals, and data structures‚Äîwhereupon they will coalesce, smoothing the edges of your programming prowess.\n\n3. **Slather on the Libraries**: Much like a charcuterie board adorned with various cheeses, enrich your knowledge with libraries such as NumPy for numerical spells, Pandas for data manipulation, and Flask for web wizardry. Each library is a piece of cheese inviting you to experience delight‚Äîa Roquefort of functionality, if you will.\n\n4. **Become a Maestro of Debugging**: Tread carefully like a fine cheese wheel upon a wobbly surface. Embrace the art of debugging; for in understanding the errors, you transform them into opportunities‚Äîakin to turning cottage cheese into something much more profound.\n\n5. **Collaborate in the Cheese Cellar**: Join communities, a veritable cheese market of knowledge! Engage with peers, partake in forums, or challenge your mettle with online coding competitions. Remember, even the sharpest cheddar may need a companion to bring out its flavor, so share and learn in harmony.\n\n6. **Partake of the Ancient Tomes**: Read tomes and scrolls of knowledge‚Äîtextbooks, online courses, and tutorials. Just as a fine Camembert matures in bright air, so too does your skill blossom when immersed in the lore of Python programming.\n\n7. **Create a Feast of Projects**: Summon forth your grand creations! Build projects that excite and inspire ‚Äì be it a web app, a game, or a simple script to automate dull tasks. Each project will be a new wheel of cheese, ripe for sharing with the world.\n\n8. **Embrace the Layered Complexity of Fondue**: Start simple, and gradually dip into more complex waters, like melting a blend of cheeses to create a divine fondue. Challenge yourself with algorithms and data structures, and meld them into the fabric of your understanding.\n\nVenture forth, young apprentice, for the path of the Python is long, yet deliciously rewarding. In all things, remember that great programming is like fine cheese; it takes time to develop richness, depth, and character. Now go forth, and let your endeavors be as legendary as the finest of Gouda!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 624, 'prompt_tokens': 50, 'total_tokens': 674, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b376dfbbd5', 'finish_reason': 'stop', 'logprobs': None}, id='run-6a92be58-3be5-4462-b648-c5dad823c1b1-0', usage_metadata={'input_tokens': 50, 'output_tokens': 624, 'total_tokens': 674, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})
Notice how we specifically referenced our content format option!

Now that we have the basics set up - let's see what we mean by "Retrieval Augmented" Generation.

Naive RAG - Manually adding context through the Prompt Template
Let's look at how our model performs at a simple task - defining what LangChain is!

We'll redo some of our previous work to change the system_template to be less...verbose.

system_template = "You are a helpful assistant."
human_template = "{content}"

chat_prompt = ChatPromptTemplate.from_messages([
    ("system", system_template),
    ("human", human_template)
])

chat_chain = chat_prompt | openai_chat_model ### LCEL Chain!

print(chat_chain.invoke({"content" : "Please define LangGraph."}))
content="LangGraph is a framework designed to facilitate the development and utilization of applications involving natural language processing (NLP) and machine learning. It enables users to build, manage, and query graphs that represent relationships and entities derived from language data. The core idea behind LangGraph is to leverage the power of graph structures to model the complexities of human language in a way that is both interpretable and scalable.\n\nKey features of LangGraph may include:\n\n1. **Graph Representation**: Using nodes to represent entities (like words, phrases, or concepts) and edges to represent relationships (like semantic connections or syntactic structures).\n\n2. **Query Language**: A specialized query language that allows users to extract insights from the graph, enabling advanced querying capabilities for complex language tasks.\n\n3. **Integration with NLP Tools**: Support for incorporating various NLP techniques, such as entity recognition, sentiment analysis, and topic modeling, to enhance the graph's richness.\n\n4. **Flexibility and Extensibility**: The architecture is often designed to be flexible, allowing developers to extend it for specific use cases, such as chatbots, knowledge graphs, or recommendation systems based on textual data.\n\n5. **Visualizations**: Tools for visualizing the graph structure and data, helping users to better understand the relationships and information contained within their datasets.\n\n6. **Application Use-Cases**: Common applications may include semantic search, question answering, and conversational AI systems.\n\nOverall, LangGraph aims to bridge the gap between machine learning, linguistics, and graph theory, providing a robust platform for natural language understanding and processing. \n\n(Note: LangGraph could refer to different projects or concepts, so verify the specific context in which it is being mentioned for accurate details.)" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 348, 'prompt_tokens': 22, 'total_tokens': 370, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_b376dfbbd5', 'finish_reason': 'stop', 'logprobs': None} id='run-cae2c6cf-430e-4bde-9eb2-902c7e7ddaae-0' usage_metadata={'input_tokens': 22, 'output_tokens': 348, 'total_tokens': 370, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}
Well, that's not very good - is it!

The issue at play here is that our model was not trained on the idea of "LangChain", and so it's left with nothing but a guess - definitely not what we want the answer to be!

Let's ask another simple LangChain question!

print(chat_chain.invoke({"content" : "What is LangChain Expression Language (LECL)?"}))
content='LangChain Expression Language (LECL) is a specialized language designed for use within the LangChain framework, which is primarily focused on building applications that leverage language models and their related capabilities. LECL provides a way to define and manipulate complex expressions in a way that integrates with the capabilities of language models, making it easier to work with data, processing, and logical operations in applications that utilize natural language processing.\n\nKey features of LECL may include:\n\n1. **Expression Evaluation**: The ability to evaluate expressions that incorporate variables, functions, and operations, enabling dynamic and flexible computations.\n\n2. **Integration with Language Models**: LECL is specifically tailored to work effectively with language models, allowing developers to access model capabilities and integrate them into their expressions seamlessly.\n\n3. **Data Manipulation**: Tools to manipulate data structures and perform operations that are common in natural language applications, such as text transformations, parsing, or extraction.\n\n4. **Ease of Use**: The syntax and structure of LECL are likely designed to be user-friendly for developers who may not be experts in programming but are familiar with working in language-centric contexts.\n\n5. **Extensibility**: LECL can potentially be extended with custom functions or operations tailored to specific applications or use cases in natural language processing.\n\nFor a developer working within the LangChain ecosystem, LECL provides a powerful toolset to harness the capabilities of language models in a structured and concise manner. It allows for more sophisticated interactions with data and enhances the overall functionality of applications built using LangChain. \n\nAs of the last update in October 2023, if you need more specific implementation details or examples, it would be best to consult the LangChain documentation or related resources for the latest information and usage guidelines.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 352, 'prompt_tokens': 27, 'total_tokens': 379, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_44added55e', 'finish_reason': 'stop', 'logprobs': None} id='run-287e8a0f-184f-4184-8342-aa32e59a8e18-0' usage_metadata={'input_tokens': 27, 'output_tokens': 352, 'total_tokens': 379, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}
While it provides a confident response, that response is entirely ficticious! Not a great look, OpenAI!

However, let's see what happens when we rework our prompts - and we add the content from the docs to our prompt as context.

HUMAN_TEMPLATE = """
#CONTEXT:
{context}

QUERY:
{query}

Use the provide context to answer the provided user query. Only use the provided context to answer the query. If you do not know the answer, or it's not contained in the provided context response with "I don't know"
"""

CONTEXT = """
LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):

Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.

Fallbacks The non-determinism of LLMs makes it important to be able to handle errors gracefully. With LCEL you can easily attach fallbacks to any chain.

Parallelism Since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.

Seamless LangSmith Tracing Integration As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximal observability and debuggability.
"""

chat_prompt = ChatPromptTemplate.from_messages([
    ("human", HUMAN_TEMPLATE)
])

chat_chain = chat_prompt | openai_chat_model

print(chat_chain.invoke({"query" : "What is LangChain Expression Language?", "context" : CONTEXT}))
content='LangChain Expression Language (LCEL) is a declarative way to easily compose chains together. It offers several benefits, including automatic support for sync, async, batch, and streaming operations, which facilitates easy prototyping and deployment. LCEL allows for graceful error handling with fallbacks, supports running components in parallel due to the nature of LLM applications, and integrates seamlessly with LangSmith for enhanced tracing and observability of complex chains.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 88, 'prompt_tokens': 282, 'total_tokens': 370, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_44added55e', 'finish_reason': 'stop', 'logprobs': None} id='run-990c4513-dd5e-41e4-94c4-2c114bf574d2-0' usage_metadata={'input_tokens': 282, 'output_tokens': 88, 'total_tokens': 370, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}
You'll notice that the response is much better this time. Not only does it answer the question well - but there's no trace of confabulation (hallucination) at all!

NOTE: While RAG is an effective strategy to help ground LLMs, it is not nearly 100% effective. You will still need to ensure your responses are factual through some other processes

That, in essence, is the idea of RAG. We provide the model with context to answer our queries - and rely on it to translate the potentially lengthy and difficult to parse context into a natural language answer!

However, manually providing context is not scalable - and doesn't really offer any benefit.

Enter: Retrieval Pipelines.

Implement Naive RAG using LCEL
Now we can make a naive RAG application that will help us bridge the gap between our Pythonic implementation and a fully LangChain powered solution!

Putting the R in RAG: Retrieval 101
In order to make our RAG system useful, we need a way to provide context that is most likely to answer our user's query to the LLM as additional context.

Let's tackle an immediate problem first: The Context Window.

All (most) LLMs have a limited context window which is typically measured in tokens. This window is an upper bound of how much stuff we can stuff in the model's input at a time.

Let's say we want to work off of a relatively large piece of source data - like the Ultimate Hitchhiker's Guide to the Galaxy. All 898 pages of it!

NOTE: It is recommended you do not run the following cells, they are purely for demonstrative purposes.

context = """
EVERY HITCHHIKER'S GUIDE BOOK
"""
We can leverage our tokenizer to count the number of tokens for us!

import tiktoken

enc = tiktoken.encoding_for_model("gpt-4o")
len(enc.encode(context))
636144
The full set comes in at a whopping 636,144 tokens.

So, we have too much context. What can we do?

Well, the first thing that might enter your mind is: "Use a model with more context window", and we could definitely do that! However, even gpt-4-128k wouldn't be able to fit that whole text in the context window at once.

So, we can try splitting our document up into little pieces - that way, we can avoid providing too much context.

We have another problem now.

If we split our document up into little pieces, and we can't put all of them in the prompt. How do we decide which to include in the prompt?!

NOTE: Content splitting/chunking strategies are an active area of research and iterative developement. There is no "one size fits all" approach to chunking/splitting at this moment. Use your best judgement to determine chunking strategies!

In order to conceptualize the following processes - let's create a toy context set!

TextSplitting aka Chunking
We'll use the RecursiveCharacterTextSplitter to create our toy example.

It will split based on the following rules:

Each chunk has a maximum size of 100 tokens
It will try and split first on the \n\n character, then on the \n, then on the <SPACE> character, and finally it will split on individual tokens.
Let's implement it and see the results!

import tiktoken
from langchain.text_splitter import RecursiveCharacterTextSplitter

def tiktoken_len(text):
    tokens = tiktoken.encoding_for_model("gpt-4o-mini").encode(
        text,
    )
    return len(tokens)

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 100,
    chunk_overlap = 0,
    length_function = tiktoken_len,
)
chunks = text_splitter.split_text(CONTEXT)
len(chunks)
3
for chunk in chunks:
  print(chunk)
  print("----")
LangChain Expression Language or LCEL is a declarative way to easily compose chains together. There are several benefits to writing chains in this manner (as opposed to writing normal code):

Async, Batch, and Streaming Support Any chain constructed this way will automatically have full sync, async, batch, and streaming support. This makes it easy to prototype a chain in a Jupyter notebook using the sync interface, and then expose it as an async streaming interface.
----
Fallbacks The non-determinism of LLMs makes it important to be able to handle errors gracefully. With LCEL you can easily attach fallbacks to any chain.

Parallelism Since LLM applications involve (sometimes long) API calls, it often becomes important to run things in parallel. With LCEL syntax, any components that can be run in parallel automatically are.
----
Seamless LangSmith Tracing Integration As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximal observability and debuggability.
----
As is shown in our result, we've split each section into 100 token chunks - cleanly separated by \n\n characters!

üèóÔ∏è Activity #1:
While there's nothing specifically wrong with the chunking method used above - it is a naive approach that is not sensitive to specific data formats.

Brainstorm some ideas that would split large single documents into smaller documents.

YOUR IDEA HERE
YOUR IDEA HERE
YOUR IDEA HERE
Embeddings and Dense Vector Search
Now that we have our individual chunks, we need a system to correctly select the relevant pieces of information to answer our query.

This sounds like a perfect job for embeddings!

We'll be using OpenAI's text-embedding-3 model as our embedding model today!

Let's load it up through LangChain.

from langchain_openai.embeddings import OpenAIEmbeddings

embedding_model = OpenAIEmbeddings(model="text-embedding-3-small")
‚ùì Question #2:
What is the embedding dimension, given that we're using text-embedding-3-small?

You will need to fill the next cell out correctly with your embedding dimension for the rest of the notebook to run.

HINT: Check out the docs to help you answer this question.

embedding_dim =  # YOUR ANSWER HERE
Using A Vector Database - Intoduction to Qdrant
Up to this point, we've been using a dictionary to hold our embeddings - typically, we'll want to use a more robust strategy.

In this bootcamp - we'll be focusing on leveraging Qdrant's vector database.

Let's take a look at how we set-up Qdrant!

NOTE: We'll be spending a lot of time learning about Qdrant throughout the remainder of our time together - but for an initial primer, please check out this resource

We are going to be using an "in-memory" Qdrant client, which means that our vectors will be held in our system's memory (RAM) - this is useful for prototyping and developement at smaller scales - but would need to be modified when moving to production. Luckily for us, this modification is trivial!

NOTE: While LangChain uses the terminology "VectorStore" (also known as a Vector Library), Qdrant is a "Vector Database" - more info. on that here.

from langchain_qdrant import QdrantVectorStore
from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams

client = QdrantClient(":memory:")
Next, we need to create a collection - a collection is a specific...collection of vectors within the Qdrant client.

These are useful as they allow us to create multiple different "warehouses" in a single client, which can be leveraged for personalization and more!

Also notice that we define what our vector shapes are (embedding dim) as well as our desired distance metric.

client.create_collection(
    collection_name="lcel_doc_v1",
    vectors_config=VectorParams(size=embedding_dim, distance=Distance.COSINE),
)
True
Now we can assemble our vector database! Notice that we provide our client, our created collection, and our embedding model!

vector_store = QdrantVectorStore(
    client=client,
    collection_name="lcel_doc_v1",
    embedding=embedding_model,
)
Now that we have our vector database set-up

_ = vector_store.add_texts(texts=chunks)
Creating a Retriever
Now that we have an idea of how we're getting our most relevant information - let's see how we could create a pipeline that would automatically extract the closest chunk to our query and use it as context for our prompt!

This will involve a popular LangChain interace known as as_retriever!

NOTE: We can still specify how many documents we wish to retrieve per vector.

retriever = vector_store.as_retriever(search_kwargs={"k": 2})
Bringing it All Together
Now that we have our Retriever, our promt Augmentation, and our Generator - we're ready to create a simple RAG chain using LCEL!

This chain does the following things:

It takes in some str and passes it to two different LCEL Runnables:
retriever, which takes the string and calls retrieve on it - passing the output (formatted as a list) to the dict under the key context
RunnablePassthrough() which simply propogates the str to the dict under the key query.
It chains the dict to format the chat_prompt which expects both a query and context
It chains the resulting message to the LLM, and calls it - returning a full response
That response is chained to the StrOutputParser() which converts the response blob into a str containing the content of the response!
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

simple_rag  = (
    {"context": retriever, "query": RunnablePassthrough()}
    | chat_prompt
    | openai_chat_model
    | StrOutputParser()
)
simple_rag.invoke("What is LCEL?")
'LCEL, or LangChain Expression Language, is a declarative way to easily compose chains together. It offers several benefits, including automatic support for async, batch, and streaming operations, making it convenient for prototyping in environments like Jupyter notebooks. It also allows for graceful error handling through fallbacks and enables parallel execution of components for efficient processing, particularly in applications involving long API calls.'
‚ùì Question #3:
What does LCEL do that makes it more reliable at scale?

HINT: Use your newly created simple_rag to help you answer this question!

A Note On Runnables
Understanding LangChain Runnables and LCEL
In LangChain, a Runnable is like a LEGO brick in your AI application - it's a standardized component that can be easily connected with other components. The real power of Runnables comes from their ability to be combined in flexible ways using LCEL (LangChain Expression Language).

Key Features of Runnables
1. Universal Interface
Every Runnable in LangChain follows the same pattern:

Takes an input
Performs some operation
Returns an output
This consistency means you can treat different components (like models, retrievers, or parsers) in the same way.

2. Built-in Parallelization
Runnables come with methods for handling multiple inputs efficiently:

# Process inputs in parallel, maintain order
results = chain.batch([input1, input2, input3])

# Process inputs as they complete
for result in chain.batch_as_completed([input1, input2, input3]):
    print(result)
3. Streaming Support
Perfect for responsive applications:

# Stream outputs as they're generated
for chunk in chain.stream({"query": "Tell me a story"}):
    print(chunk, end="", flush=True)
4. Easy Composition
The | operator makes building pipelines intuitive:

# Create a basic RAG chain
rag_chain = retriever | prompt | model | output_parser
Common Types of Runnables
Language Models: Like our ChatOpenAI instance
Prompt Templates: Format inputs consistently
Retrievers: Get relevant context from a vector store
Output Parsers: Structure model outputs
LangGraph Nodes: Individual components in our graph
Think of Runnables as the building blocks of your LLM application. Just like how you can combine LEGO bricks in countless ways, you can mix and match Runnables to create increasingly sophisticated applications!

ü§ù Breakout Room #2
LangGraph Based RAG
Now that we have a reasonable grasp of LCEL and the idea of Runnables - let's see how we can use LangGraph to build the same system!

Primer: What is LangGraph?
LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.

Why Cycles?
In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.

Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effectively allowing us to recreate application flowcharts in code in an almost 1-to-1 fashion.

Why LangGraph?
Beyond the agent-forward approach - we can easily compose and combine traditional "DAG" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!

NOTE: We're going to focus on building a simple DAG for today's assignment as an introduction to LangGraph

#!pip install -qU langgraph
Putting the State in Stateful
Earlier we used this phrasing:

coordinated multi-actor and stateful applications

So what does that "stateful" mean?

To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.

LangGraph leverages a StatefulGraph which uses an AgentState object to pass information between the various nodes of the graph.

There are more options than what we'll see below - but this AgentState object is one that is stored in a TypedDict with the key messages and the value is a Sequence of BaseMessages that will be appended to whenever the state changes.

However, in our example here, we're focusing on a simpler State object:

class State(TypedDict):
    question: str
    context: list[Document]
    response: str
Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):

We initialize our state object:

{
    "question": "",
    "context": [],
    "response": ""
}
Our user submits a query to our application.
We store the user's question in state["question"]. Now we have:

{
    "question": "How tall is the Eiffel Tower?",
    "context": [],
    "response": ""
}
We pass our state object to an Agent node which is able to read the current state. It will use the value of state["question"] as input and might retrieve some context documents related to the question. It then generates a response which it stores in state["response"]. For example:

{
    "question": "How tall is the Eiffel Tower?",
    "context": [Document(page_content="...some data...")],
    "response": "The Eiffel Tower is about 324 meters tall..."
}
That's it! The important part is that we have a consistent object (State) that's passed around, holding the crucial information as we go from one node to the next. This ensures our application has a single source of truth about what has happened so far and what is happening now.

from langgraph.graph import START, StateGraph
from typing_extensions import TypedDict
from langchain_core.documents import Document

class State(TypedDict):
  question: str
  context: list[Document]
  response: str
Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!

Let's take a second to refresh ourselves about what a graph is in this context.

Graphs, also called networks in some circles, are a collection of connected objects.

The objects in question are typically called nodes, or vertices, and the connections are called edges.

Let's look at a simple graph.

image

Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.

If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL Runnable.

If we were to think about edges in the context of LangGraph - we might think of them as "paths to take" or "where to pass our state object next".

Building Nodes
We're going to need two nodes:

A node for retrieval, and a node for generation.

Let's start with our retrieve node!

Notice how we do not need to update the state object in the node, but can instead return a modification directly to our state.

def retrieve(state: State) -> State:
  retrieved_docs = retriever.invoke(state["question"])
  return {"context" : retrieved_docs}
Next, let's create our generate node - which will leverage some LCEL!

def generate(state: State) -> State:
  generation_chain = chat_prompt | openai_chat_model | StrOutputParser()
  response = generation_chain.invoke({"query" : state["question"], "context" : state["context"]})
  return {"response" : response}
Now we can start defining our graph!

Think of the graph's state as a blank canvas that we can add nodes and edges to.

Every graph starts with two special nodes - START and END - the act as the entry and exit point to the other nodes in the graphs.

All valid graphs must start at the START node and end at the END node.

# Start with the blank canvas
graph_builder = StateGraph(State)
Now we can add a sequence to our "canvas" (graph) - this can be done by providing a list of nodes, the will automatically have edges that connect the i-th element to the i+1-th element in the list. The final element will be added to the END node unless otherwise specified.

graph_builder = graph_builder.add_sequence([retrieve, generate])
Next, let's connect our START node to our retrieve node by adding an edge.

graph_builder.add_edge(START, "retrieve")
<langgraph.graph.state.StateGraph at 0x7f0516c35250>
Finally we can compile our graph! This will do basic verification to ensure that the Runnables have the correct inputs/outputs and can be matched.

graph = graph_builder.compile()
Finally, we can visualize our graph!

graph

Let's take it for a spin!

We invoke our graph like we do any other Runnable in LCEL!

NOTE: That's right, even a compiled graph is a Runnable!

response = graph.invoke({"question" : "How does LCEL work?"})
response["response"]
'LCEL, or LangChain Expression Language, is a declarative method for composing chains together. It offers several benefits, including:\n\n1. **Async, Batch, and Streaming Support**: Chains constructed using LCEL automatically support synchronous, asynchronous, batch, and streaming operations. This allows for easy prototyping in environments like Jupyter notebooks and the capability to expose these chains as async streaming interfaces.\n\n2. **Error Handling with Fallbacks**: Due to the non-deterministic nature of large language models (LLMs), LCEL allows for the easy attachment of fallbacks to any chain, enabling graceful error handling.\n\n3. **Parallelism**: LCEL supports parallel execution for components that can run concurrently, which is particularly useful for LLM applications that involve potentially lengthy API calls.\n\nOverall, LCEL enhances the flexibility and reliability of building and managing chains in applications involving LLMs.'
response = graph.invoke({"question" : "Who is Batman?"})
response["response"]
"I don't know."
‚ùì Question #4:
LangGraph's graph-based approach lets us visualize and manage complex flows naturally. How could we extend our current implementation to handle edge cases? For example:

What if the retriever finds no relevant context?
What if the response needs fact-checking? Consider how you would modify the graph to handle these scenarios.
